{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b47c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def convolution_transpose(matrix, in_channels, out_channels, kernel_size,\n",
    "                          stride=1, padding=0, output_padding=0, dilation=1,\n",
    "                          bias=True, padding_mode='zeros'):\n",
    "    # Генерация случайных значений для смещения (bias) или заполнение нулями\n",
    "    bias_val = torch.rand(out_channels) if bias else torch.zeros(out_channels)\n",
    "\n",
    "    # Генерация случайных значений для весов или создание с указанными размерами\n",
    "    weights = torch.rand(in_channels, out_channels, kernel_size, kernel_size) if isinstance(kernel_size, int) else torch.rand(in_channels, out_channels, *kernel_size)\n",
    "\n",
    "    # Список для хранения результатов тензоров\n",
    "    res_tensor = []\n",
    "\n",
    "    # Итерация по выходным каналам\n",
    "    for l in range(out_channels):\n",
    "        # Инициализация feature_map нулями\n",
    "        feature_map = torch.zeros((matrix.shape[1] - 1) * stride + dilation * (kernel_size - 1) + 1,\n",
    "                                  (matrix.shape[2] - 1) * stride + dilation * (kernel_size - 1) + 1)\n",
    "\n",
    "        # Итерация по входным каналам\n",
    "        for c in range(in_channels):\n",
    "            for i in range(0, matrix.shape[1]):\n",
    "                for j in range(0, matrix.shape[2]):\n",
    "                    # Получение значения из входной матрицы и соответствующего веса\n",
    "                    val = matrix[c][i][j]\n",
    "                    proizv = val * weights[c][l]\n",
    "\n",
    "                    # Инициализация zero_tensor для текущего значения\n",
    "                    zero_tensor = torch.zeros((weights.shape[2] - 1) * dilation + 1,\n",
    "                                              (weights.shape[3] - 1) * dilation + 1)\n",
    "\n",
    "                    # Итерация для установки значений в zero_tensor\n",
    "                    for a in range(0, zero_tensor.shape[0], dilation):\n",
    "                        for b in range(0, zero_tensor.shape[1], dilation):\n",
    "                            zero_tensor[a][b] = proizv[a // dilation][b // dilation]\n",
    "\n",
    "                    # Добавление zero_tensor к соответствующей области feature_map\n",
    "                    res = np.add(zero_tensor,\n",
    "                                 feature_map[i * stride:i * stride + (weights.shape[2] - 1) * dilation + 1,\n",
    "                                 j * stride:j * stride + (weights.shape[3] - 1) * dilation + 1])\n",
    "                    feature_map[i * stride:i * stride + (weights.shape[2] - 1) * dilation + 1,\n",
    "                    j * stride:j * stride + (weights.shape[3] - 1) * dilation + 1] = res\n",
    "\n",
    "        # Добавление смещения (bias) и обрезка feature_map\n",
    "        res_tensor.append(np.add(feature_map, np.full(feature_map.shape, bias_val[l])))\n",
    "\n",
    "    # Применение output_padding и обрезка для каждого тензора в результате\n",
    "    for t in range(len(res_tensor)):\n",
    "        if output_padding > 0:\n",
    "            pad_func = torch.nn.ConstantPad1d((0, output_padding, 0, output_padding), 0)\n",
    "            res_tensor[t] = pad_func(res_tensor[t])\n",
    "\n",
    "        res_tensor[t] = res_tensor[t][0 + padding:res_tensor[t].shape[0] - padding,\n",
    "                                      0 + padding:res_tensor[t].shape[1] - padding]\n",
    "\n",
    "    return res_tensor, weights, torch.tensor(bias_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6678cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor:\n",
      "tensor([[0.9027, 1.9098, 0.9352, 2.5715, 1.1801, 2.9071, 0.9434, 1.4642],\n",
      "        [2.0210, 3.6973, 1.8302, 4.5274, 3.9776, 5.1546, 2.7033, 2.3064],\n",
      "        [1.0215, 1.9005, 0.6667, 2.0264, 0.8654, 2.3572, 1.0171, 1.7830],\n",
      "        [2.7187, 3.6481, 2.0825, 4.4231, 3.3661, 5.2843, 3.6773, 3.0200],\n",
      "        [0.7599, 1.9878, 0.7391, 2.3780, 0.9574, 2.9887, 1.2266, 2.0940],\n",
      "        [2.8173, 4.4106, 3.0571, 4.5421, 3.1799, 4.6192, 3.3037, 2.2478],\n",
      "        [1.0212, 2.6195, 0.9294, 2.3941, 0.7755, 1.9281, 0.8717, 1.2733],\n",
      "        [1.6829, 2.8950, 1.9624, 2.3634, 1.3139, 1.8695, 1.3328, 0.8618]])\n",
      "tensor([[1.6182, 1.9939, 1.6253, 2.2668, 2.5872, 2.8660, 1.8212, 1.3270],\n",
      "        [2.1402, 2.2587, 1.4978, 3.3080, 2.9704, 3.7707, 2.2100, 1.9179],\n",
      "        [1.8631, 1.7294, 1.2127, 1.9745, 1.9138, 2.2344, 2.1790, 1.9547],\n",
      "        [2.0346, 2.9921, 1.6645, 3.1610, 2.4412, 3.4814, 3.3167, 2.5313],\n",
      "        [1.6997, 2.0906, 1.5906, 2.3979, 2.1951, 3.0739, 2.5839, 1.8983],\n",
      "        [2.6819, 3.5279, 2.4585, 3.3526, 2.5251, 3.3238, 2.7217, 2.4988],\n",
      "        [1.9635, 2.1491, 2.0781, 2.3718, 1.5157, 1.8638, 1.6045, 1.0609],\n",
      "        [1.3372, 2.5345, 1.6786, 2.3724, 1.0783, 1.6125, 1.0516, 1.3352]])\n",
      "\n",
      "Weights:\n",
      "tensor([[[[0.3014, 0.9963, 0.2849],\n",
      "          [0.5112, 0.0044, 0.3907],\n",
      "          [0.9465, 0.7502, 0.2632]],\n",
      "\n",
      "         [[0.8352, 0.1778, 0.1290],\n",
      "          [0.0962, 0.5582, 0.7588],\n",
      "          [0.7985, 0.6765, 0.6075]]],\n",
      "\n",
      "\n",
      "        [[[0.9845, 0.0611, 0.7963],\n",
      "          [0.2188, 0.3730, 0.7597],\n",
      "          [0.6372, 0.8612, 0.8883]],\n",
      "\n",
      "         [[0.0383, 0.3906, 0.1584],\n",
      "          [0.3275, 0.8397, 0.7941],\n",
      "          [0.1750, 0.8072, 0.9165]]],\n",
      "\n",
      "\n",
      "        [[[0.5852, 0.9403, 0.5962],\n",
      "          [0.7716, 0.4507, 0.7746],\n",
      "          [0.9913, 0.7163, 0.2512]],\n",
      "\n",
      "         [[0.1935, 0.9767, 0.1976],\n",
      "          [0.8916, 0.9775, 0.2102],\n",
      "          [0.1286, 0.2221, 0.5936]]]])\n",
      "\n",
      "Bias Values:\n",
      "tensor([0.4693, 0.5920])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pospi\\AppData\\Local\\Temp\\ipykernel_12772\\3736592967.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return res_tensor, weights, torch.tensor(bias_val)\n"
     ]
    }
   ],
   "source": [
    "# Создаем тестовый тензор\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Создаем входной тензор\n",
    "input_tensor = torch.rand(3, 4, 4)\n",
    "\n",
    "# Параметры для ConvTranspose\n",
    "in_channels = 3\n",
    "out_channels = 2\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "output_padding = 1\n",
    "dilation = 1\n",
    "bias = True\n",
    "padding_mode = 'zeros'\n",
    "\n",
    "# Вызываем custom_conv_transpose\n",
    "output_result, weights, bias_values = custom_conv_transpose(input_tensor, in_channels, out_channels, kernel_size,\n",
    "                                                             stride, padding, output_padding, dilation,\n",
    "                                                             bias, padding_mode)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Output Tensor:\")\n",
    "for tensor in output_result:\n",
    "    print(tensor)\n",
    "\n",
    "print(\"\\nWeights:\")\n",
    "print(weights)\n",
    "\n",
    "print(\"\\nBias Values:\")\n",
    "print(bias_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b900cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import unittest\n",
    "\n",
    "class TestConvolutionTranspose(unittest.TestCase):\n",
    "\n",
    "    def test_convolution_transpose_no_bias(self):\n",
    "        input_matrix = torch.randn(2, 4, 4)  # 2 channels, 4x4 matrix\n",
    "        in_channels = 2\n",
    "        out_channels = 3\n",
    "        kernel_size = 2\n",
    "        stride = 2\n",
    "        padding = 1\n",
    "        output_padding = 0\n",
    "        dilation = 1\n",
    "        bias = False\n",
    "        padding_mode = 'zeros'\n",
    "\n",
    "        my_res, kernel, bias_val = custom_conv_transpose(\n",
    "            input_matrix,\n",
    "            in_channels=in_channels, out_channels=out_channels,\n",
    "            kernel_size=kernel_size, stride=stride,\n",
    "            padding=padding, output_padding=output_padding,\n",
    "            dilation=dilation, bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "        )\n",
    "\n",
    "        # Check if the PyTorch convolution transpose layer has bias\n",
    "        if bias:\n",
    "            # If bias is True, use the provided bias value\n",
    "            torch_function = torch.nn.ConvTranspose2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=kernel_size, stride=stride,\n",
    "                padding=padding, output_padding=output_padding,\n",
    "                dilation=dilation, bias=bias,\n",
    "                padding_mode=padding_mode,\n",
    "            )\n",
    "            torch_function.weight.data = kernel\n",
    "            torch_function.bias.data = bias_val\n",
    "        else:\n",
    "            # If bias is False, create a ConvTranspose2d layer without bias\n",
    "            torch_function = torch.nn.ConvTranspose2d(\n",
    "                in_channels=in_channels, out_channels=out_channels,\n",
    "                kernel_size=kernel_size, stride=stride,\n",
    "                padding=padding, output_padding=output_padding,\n",
    "                dilation=dilation, bias=False,  # Set bias to False\n",
    "                padding_mode=padding_mode,\n",
    "            )\n",
    "            torch_function.weight.data = kernel\n",
    "\n",
    "        result = str(np.round([tensor.tolist() for tensor in my_res], 2))\n",
    "        torch_res = str(np.round(torch_function(input_matrix).data.numpy(), 2))\n",
    "\n",
    "        # Corrected indentation for the assertion\n",
    "        self.assertEqual(result, torch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0345074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pospi\\AppData\\Local\\Temp\\ipykernel_12772\\3736592967.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return res_tensor, weights, torch.tensor(bias_val)\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.016s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x28ab6707370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e32c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
